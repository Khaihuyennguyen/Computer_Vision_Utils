{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import sys\n",
    "import os.path\n",
    "import matplotlib.pyplot as plt\n",
    "# matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (15.0,15.0)\n",
    "matplotlib.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Initialize the parameters\n",
    "\n",
    "The YOLO3 algorithm generates bounding boxes as the predicted detection outputs. Every predicted box is associated with a confidience score. In the first stage, all the boxes below the confidence threshold parameter are ignored for futher processing.\n",
    "\n",
    " The rest of the boxes undergo non-maxinum suppression which removes redundant overlapping bounding boxes. Non-maximum suppression is controlled by a parameter nmsThreshold. You can try to change these values and see how the numkber of the output predicted boxes changes.\n",
    " \n",
    "Next, the default values for the input width and height for the networkd input image are set. We set each of them to 416 so that we can compare "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the parameters\n",
    "objectnessThreshold = 0.5 # Objectness threshold\n",
    "confThreshold = 0.5       # Confidence threshold\n",
    "nmsThreshold = 0.4        # Non-maximum suppression threshold\n",
    "inpWidth = 416            # Width of network's input image\n",
    "inpHeight = 416           # Height of network's input image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Load the model and classes \n",
    "\n",
    "The file coco.names contains all the objects for which the model was trained. We read class names\n",
    "\n",
    "Next, we load the network which has two part\n",
    "\n",
    "- Yolo weights and yolo conf\n",
    "\n",
    "We set the DNN backend to OpenCV here and the target CPU. You could try setting the preferable target to cv.dnn.DNN_TARGET_OPENCL to run it on a GPU. But keep in mind that the current OpenCV version is tested only with Intelâ€™s GPUs, it would automatically switch to CPU, if you do not have an Intel GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load names of classes\n",
    "classesFile = \"coco.names\"\n",
    "classes = None\n",
    "\n",
    "with open(classesFile, 'rt') as f:\n",
    "    classes = f.read().rstrip('\\n').split('\\n')\n",
    "    \n",
    "# Give the configuration and weights files for the model and load the network using them\n",
    "modelConfiguration = \"yolov3.cfg\"\n",
    "modelWeights = \"yolov3.weights\"\n",
    "\n",
    "net = cv2.dnn.readNetFromDarknet(modelConfiguration, modelWeights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Process each frame\n",
    "\n",
    "The input image is passed through the network and the output is decoded and displayed using a few utility functions. Let us go over the utility functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3a: Getting the names of output layers\n",
    "\n",
    "The forward function in OpenCV's Net class needs the ending layer till which it should run in the network. Since we want to run through the whole network, we need to identify the last layer of the network. We do that by uisng the functiongetUnconnectedOutlayers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOutputsNames(net):\n",
    "    # Get the names of all the layers in the network\n",
    "    layersNames = net.getLayerNames()\n",
    "    return [layersNames[i[0] - 1] for i in net.getUnconnectedOutLayers()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the predicted boundingb ox\n",
    "def drawPred (classId, conf, left, top, right, bottom):\n",
    "    # Draw a bounding box\n",
    "    cv2.rectangle(frame, (left, top), (right, bottom), (255, 178, 50), 3)\n",
    "    label = '%.2f' % conf\n",
    "    \n",
    "    # Get the label for the class name and its confidence\n",
    "    if classes:\n",
    "        assert(classId < len(classes))\n",
    "        label = '%s:%s' % (classes[classId], label)\n",
    "    # Display the label at the top of the bouding box\n",
    "    labelSize, baseLine = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n",
    "    top = max(top, labelSize[1])\n",
    "    top = max(top, labelSize[1])\n",
    "    cv2.rectangle(frame,(left, top - round(1.5*labelSize[1])), (left + round(1.5*labelSize[0]), top + baseLine), (255, 255, 255), cv2.FILLED)\n",
    "    cv2.putText(frame, label, (left, top), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0,0,0), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3c: Post-processing the network's output \n",
    "The network outputs bounding boxes are each represented by a vector of number of classes and 5 elements.\n",
    "\n",
    "The first 4 elements represent the center_x; center_y, width and height. The fifth element represents the confidence that the bouding box encloses an object.\n",
    "\n",
    "The rest of the elements are the confidence associated with each class (i.e object type) The box is assigned to the class corresponding to the hightest scence for the box\n",
    "\n",
    "The highest score for a box is also called its confidence. If the confidence of a box is less than the given threshold, the bounding box is dropped and not considreed for further processing\n",
    "\n",
    "The boxes with their confidence equal to or greater than the threshold are then subjected to Non Maximum Suppression. This would reduce the number of overlapping boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the bounding boxes with low confidence using non-maxima suppression\n",
    "\n",
    "def postprocess(frame, outs):\n",
    "    frameHeight = frame.shape[0]\n",
    "    frameWidth = frame.shape[1]\n",
    "    \n",
    "    classIds = []\n",
    "    \n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    # scan through all the bounding boxes output from the network and keep only\n",
    "    # The ones high confidence. Assigned the label with the highest confidence\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            if detection[4] > objectnessThreshold:\n",
    "                scores = detection[5:]\n",
    "                classId = np.argmax(scores)\n",
    "                confidence = scores[classId]\n",
    "                if confidence > confThreshold:\n",
    "                    center_x = int(detection[0] * frameWidth)\n",
    "                    center_y = int(detection[1] * frameHeight)\n",
    "                    width = int(detection[2] * frameWidth)\n",
    "                    height = int(detection[3] * frameHeight)\n",
    "                    left = int(center_x - width/2)\n",
    "                    top = int(center_y - height/2)\n",
    "                    classIds.append(classId)\n",
    "                    confidences.append(float(confidences))\n",
    "                    boxes.append([left, top, width, height])\n",
    "    # Perform non maximum suppression to eliminate redundant overlapping boxes with\n",
    "    # lower confidences\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, confThreshold, nmsThreshold)\n",
    "    for i in indices:\n",
    "        i = i[0]\n",
    "        box = boxes[i]\n",
    "        left = box[0]\n",
    "        top = box[1]\n",
    "        width = box[2]\n",
    "        height = box[3]\n",
    "        drawPred(classIds[i], confidence, left, top, left + width, top + height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The non maximum Suppression is controlled by the nmsThreshold parameter. If nmsThreshold is set too low, like 0.1, we might not detect overlapping object of same or different classes. But if it is set to high like 1. We might get multiple boxes for the same object. So we used an intermidate value 0.4 in our code above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main loop\n",
    "The input image to a neural network needs to be in a certain format called a blob\n",
    "\n",
    "After a frame is road from the input image or vieo stream, it is passed through the blobFromImage function to convert it to an input blob for the neural network. In this process, it scales the image pixel values to a target range 0 1 using a scale factor 1/255. It also resizes the image to the given size of (416, 416) without cropping. Note that we do not perform any mean substraction here, hence pass[0,0,0] to the mean parameter of the function and keep the swapRB parameter to its default value of 1.\n",
    "\n",
    "The output blob is then passed in to the network as its input and a forward pass is run to get a list of predicted bounding boxes as the network's output. These boxes go through a post processing step in order filter out the ones with low confidence scores. We will go through the post processing step in more details in the next section .We preint out the inference time for each frame at the top left. The image with the final bounding boxes is then saved to the disk, other as an image for an image input using a video writer for the input video stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process inputs\n",
    "imagePath = D"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opencv-env",
   "language": "python",
   "name": "opencv-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
